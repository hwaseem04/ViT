{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange # it is a function\n",
    "\n",
    "from einops.layers.torch import Rearrange # it is used as a layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(inp):\n",
    "    return inp if isinstance(inp, tuple) else (inp, inp)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention block of each layer\n",
    "    Args:\n",
    "        dim: Input dimension to MHSA block, i.e dimension of each token corresponding to each patch\n",
    "        heads: Total attention heads \n",
    "        dim_head: Dimension of Q, K, V vector\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads, dim_head):\n",
    "        \"\"\"\n",
    "        - Recall, each self-attention layer(head) in a MHSA unit has its own Q, K, V vector.\n",
    "        - But we have several heads, if heads=8, we need 8 such vectors for each Q, K, V.\n",
    "        - We love vectorised implementation, so instead of maintaining 8 different vectors of dimension dim_head for each Q, K, V\n",
    "          we are maintaining just three vector(for Q, K, V) of dimenstion dim * dim_head, \n",
    "          which you should understand as concatenating each of the 8 Q, K, V vectors of each heads(=8 here) as a single vector for Q, K, V each. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Dimension of vector (Q/K/V) including all heads\n",
    "        inner_dim = dim_head * heads \n",
    "        self.heads = heads\n",
    "\n",
    "        # used for dividing product of Q and K vector. Refer Self attention mechanism: https://jalammar.github.io/illustrated-transformer/\n",
    "        self.scale = dim_head ** 0.5 \n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # dim=-1 because after multiplying Q and K, last dimension in that product corresponds to columns.\n",
    "        # We need to apply softmax across columns since each column corresponds to different token, and after \n",
    "        # doing softmax, it essentialy shows the probability of a token attenting to each of the other token\n",
    "        self.attend = nn.Softmax(dim=-1) \n",
    "\n",
    "        # 3 is multiplied because as I said, we need 3 vectors of each Q, K, V (each is a concatenated version for entire heads, hence innerdim)\n",
    "        # We dont need bias term, i.e \"a = Wx\" is enough\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        # To convert each of the Q, K, V vector back to encoder's dimension, so that it can be processed by FF block.\n",
    "        # Again bias is insignificant/irrelevant here\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If say crop size is 256 by 256 by 3\n",
    "        # say Patch size is 8 by 8 by 3, so h and w = 265/8 = 32 \n",
    "        # say Total batch is 15\n",
    "        # Dimension of x -> (15, h*w , dim)\n",
    "        # assume total heads=8\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Converts dimension from (15, h*w, dim) to (3, 15, h*w, inner_dim) as it is chunked into 3. \n",
    "        # Dim=-1 because actual output of to_qkv returns dimension (15, h*w, inner_dim * 3), axis=-1 corresponds to last dimension\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "        # Each of the 3 chunk of dimension (15, h*w, inner_dim) is passed into the rearrange function below\n",
    "        # n: total patches per image, i.e h*w\n",
    "        # (h d): inner_dim = heads * dim_head\n",
    "        # below einops operation seperates \"concatenated\" vector for each heads\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        # Dimension(as per above assumed numbers) of\n",
    "        # q: (15, 8, h*w, dim_head)\n",
    "        # k.transpose(-1,-2): (15, 8, dim_head, h*w)\n",
    "        # dot_prod: (15, 8, h*w, h*w)\n",
    "        dot_prod = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        print('dot_prod :', dot_prod.shape)\n",
    "        # dimension: (15, 8, h*w, h*w)\n",
    "        attn_weights = self.attend(dot_prod)\n",
    "\n",
    "        # attn_scores dimension: (15, 8, h*w, dim_head)\n",
    "        # dimension of v: (15, 8, h*w, dim_head)\n",
    "        attn_scores = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Output dimension: (15, h*w, inner_dim)\n",
    "        output = rearrange(attn_scores, 'b h n d -> b n (h d)')\n",
    "\n",
    "        # Output dimension: (15, h*w, dim)\n",
    "        output = self.to_out(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward(FF) block(Essentially MLP) inside each encoder block\n",
    "    Args:\n",
    "        dim: Input dimension to FF layer. Which is the internal dimension of each patch\n",
    "        mlp_dim: Hidden layer dimension of this MLP \n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            # Layer norm is applied here because in the forward function of transformer only residual connection is performed, layernorm is performed here instead\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Block of ViT\n",
    "    Args:\n",
    "        depth: Total layers/blocks in encoder\n",
    "        heads: Total self-attention blocks in a single MHSA unit of a single layer\n",
    "        mlp_dim: Hidden layer dimension of FeedForward network in each layer/block in encoder\n",
    "        dim_head: Dimension of Q,K,V vector\n",
    "        dim: Input dimension of each token corresponding to each patch\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, dim_head, depth, heads, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # All layers/blocks in encoder. Each layer contains a Multi-head self-attention block and MLP block\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, dim_head, heads),\n",
    "                FeedForward(dim, mlp_dim),\n",
    "            ]))\n",
    "        \n",
    "        # TODO: For quite easier view\n",
    "        #self.norm = nn.LayerNorm(dim) \n",
    "        #x = attn(self.norm(x)) + x \n",
    "        #x = attn(self.norm(x)) + x \n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            # Note: Layer norm is applied inside attn(x) and ff(x). Only residual connection is performed below. Refer architecture figure in original paper.\n",
    "            # Adding x for residual connection \n",
    "            x = attn(x) + x \n",
    "\n",
    "            # addition of MHSA block's output and its input is passed as input to ff layer\n",
    "            x = ff(x) + x \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_size: Crop dimension say 256 by 256\n",
    "        patch_size: Patchification of each crop, say 8 by 8\n",
    "        num_class: Total classes for classification\n",
    "        dim: Encoder input dimension. Each patch vector is converted to dim dimensional vector\n",
    "        depth: Total number of encoder layers\n",
    "        heads: Total number of self-attention layers in Multi-head Self-attention block of each encoder layer\n",
    "        mlp_dim: Hidden layer dimension of FeedForward network in each layer/block in encoder\n",
    "        channels: Input image channel. 3 for RGB, 1 for Grey scale.\n",
    "        dim_head: Dimension of Q, K, V vector used for Self-Attention. Q and K are usually of same dimension, V can be arbitary.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, num_class, dim, depth, heads, mlp_dim, channels=3, dim_head=64):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "\n",
    "        # For now forcing the crop image dimension to be divisible by patch size\n",
    "        # TODO: Create overlapping patches when they are not divisible\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimension should be divisible by patch size\"\n",
    "\n",
    "        # s = HW/p^2 (refer paper, where H, W are height and width of crop image, p is patch dimension-assuming it to be square)\n",
    "        num_patches = (image_height * image_width) // (patch_height * patch_width)\n",
    "        \n",
    "        # Vectorising the patch along its channels\n",
    "        patch_dim = patch_height * patch_width * channels\n",
    "\n",
    "        # Converting the vectorised patch to a fixed dimension so that it can be passed into encoder as input\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # refer Einops: https://einops.rocks/\n",
    "            # Idea is simple, if einops isn't used it is going to be a headache to maintain the dimensions, matrix multiplications etc as we are dealing with lot of different entities like image crop, image patches in batches\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "\n",
    "            # Lets say original input be (15, 3, 256, 256) and p1=p2=16\n",
    "            # After rearranging (15, 32, 32, 8 * 8 * 3)\n",
    "            nn.LayerNorm(patch_dim), # normalising across the channels (now 8 * 8 * 3)\n",
    "\n",
    "            nn.Linear(patch_dim, dim), # Converting the vectorised patches to fixed dimension to get processed by encoder\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        # Creating Encoder with multiple layers\n",
    "        self.transformer = Transformer(dim, dim_head, depth, heads, mlp_dim)\n",
    "\n",
    "        # No hidden layer in the MLP used for classification outside of encoder\n",
    "        self.linear_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_crop):\n",
    "        \"\"\"\n",
    "        Assuming batch size=15\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(image_crop)\n",
    "        # pe = ...\n",
    "        \n",
    "        # Dimension of x changed to: (15, 32*32, 8*8*3)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') \n",
    "\n",
    "        # Dimension of x returned from transformer: (15, h*w, dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # In a updated version of ViT (by same authors, referenced in README.md), \n",
    "        # class token is removed and instead the classification is made by taking mean \n",
    "        # over tokens for all patches.\n",
    "        # Dimension of x now: (15, dim)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # If total classes=2 (say binary classification problem)\n",
    "        # then dimension of x: (15, 2)\n",
    "        x = self.linear_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input = np.random.rand(15, 3, 256, 256)\n",
    "input = torch.from_numpy(input).to(torch.float32)\n",
    "\n",
    "model = ViT(\n",
    "    image_size=256,\n",
    "    patch_size=8,\n",
    "    num_class=2,\n",
    "    dim=128,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    mlp_dim=1024,\n",
    "    channels=3,\n",
    "    dim_head=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_prod : torch.Size([15, 64, 1024, 1024])\n",
      "dot_prod : torch.Size([15, 64, 1024, 1024])\n",
      "dot_prod : torch.Size([15, 64, 1024, 1024])\n",
      "dot_prod : torch.Size([15, 64, 1024, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
